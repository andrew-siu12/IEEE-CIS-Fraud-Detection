{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:56:07.757454Z",
     "start_time": "2019-08-24T08:56:06.471562Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from util import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from time import time\n",
    "import datetime\n",
    "from scipy.stats import ks_2samp\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:56:45.838364Z",
     "start_time": "2019-08-24T08:56:07.769165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 38.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_identity = pd.read_csv(\"../data/train_identity.csv\", )\n",
    "train_transaction = pd.read_csv(\"../data/train_transaction.csv\", )\n",
    "test_identity = pd.read_csv(\"../data/test_identity.csv\", )\n",
    "test_transaction = pd.read_csv(\"../data/test_transaction.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:56:46.636436Z",
     "start_time": "2019-08-24T08:56:46.629604Z"
    }
   },
   "outputs": [],
   "source": [
    "base_columns = list(train_transaction.columns) + list(train_identity.columns)\n",
    "# base model feature improtance\n",
    "feat = ['card2', 'C13', 'card1', 'TransactionAmt', 'C1', 'addr1', 'D15',\n",
    "        'D2', 'P_emaildomain', 'C14', 'card5', 'C11', 'V45', 'D8',\n",
    "        'card3', 'V313', 'D1', 'id_02', 'R_emaildomain', 'card6',\n",
    "        'id_20', 'D4', 'D10', 'DeviceInfo', 'C2', 'id_01', 'V310',\n",
    "        'V62', 'C12', 'dist1', 'V87', 'M4', 'V283', 'V281', 'V294',\n",
    "        'V258', 'C8', 'V53', 'id_09', 'V314', 'V38', 'id_30', 'V315',\n",
    "        'C6', 'V317', 'id_33', 'V312', 'V189', 'id_19', 'C4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:56:47.393793Z",
     "start_time": "2019-08-24T08:56:47.379151Z"
    }
   },
   "outputs": [],
   "source": [
    "def id_split(train_identity, test_identity):\n",
    "    \"\"\"\n",
    "    Group same mobile phone company with different build into same group, lower count of device group into others\n",
    "    Seperate the mobile phone device name and the version number into seperate features.\n",
    "    Seperate the mobile phone OS and version number into sperate features\n",
    "    Seperate the browser OS and version number into sperate feautures\n",
    "    :param train_identity: dataframe:\n",
    "    :param test_identity:  dataframe\n",
    "    :return:  updated column of train_identity and test_identity\n",
    "    \"\"\"\n",
    "    for df in [train_identity, test_identity]:\n",
    "        df['device_name'] = df['DeviceInfo'].str.split('/', expand=True)[0]\n",
    "        df['device_version'] = df['DeviceInfo'].str.split('/', expand=True)[1]\n",
    "\n",
    "        df['OS_id_30'] = df['id_30'].str.split(' ', expand=True)[0]\n",
    "        df['version_id_30'] = df['id_30'].str.split(' ', expand=True)[1]\n",
    "\n",
    "        df['browser_id_31'] = df['id_31'].str.split(' ', expand=True)[0]\n",
    "        df['version_id_31'] = df['id_31'].str.split(' ', expand=True)[1]\n",
    "\n",
    "        df['screen_width'] = df['id_33'].str.split('x', expand=True)[0]\n",
    "        df['screen_height'] = df['id_33'].str.split('x', expand=True)[1]\n",
    "\n",
    "        df['id_34'] = df['id_34'].str.split(':', expand=True)[1]\n",
    "        df['id_23'] = df['id_23'].str.split(':', expand=True)[1]\n",
    "\n",
    "        df.loc[df['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n",
    "        df.loc[df['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n",
    "        df.loc[df['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n",
    "        df.loc[df['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n",
    "        df.loc[df['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n",
    "        df.loc[df['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n",
    "        df.loc[df['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n",
    "        df.loc[df['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n",
    "        df.loc[df['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n",
    "        df.loc[df['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n",
    "        df.loc[df['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n",
    "        df.loc[df['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n",
    "        df.loc[df['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n",
    "        df.loc[df['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n",
    "        df.loc[df['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n",
    "        df.loc[df['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n",
    "        df.loc[df['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n",
    "\n",
    "        df.loc[df.device_name.isin(df.device_name.value_counts()[\n",
    "                                       df.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n",
    "        df['had_id'] = 1\n",
    "        gc.collect()\n",
    "\n",
    "    return train_identity, test_identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:56:55.714523Z",
     "start_time": "2019-08-24T08:56:48.049704Z"
    }
   },
   "outputs": [],
   "source": [
    "train_identity, test_identity = id_split(train_identity, test_identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:56:56.688342Z",
     "start_time": "2019-08-24T08:56:56.503725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 144233 entries, 0 to 144232\n",
      "Data columns (total 50 columns):\n",
      "TransactionID     144233 non-null int64\n",
      "id_01             144233 non-null float64\n",
      "id_02             140872 non-null float64\n",
      "id_03             66324 non-null float64\n",
      "id_04             66324 non-null float64\n",
      "id_05             136865 non-null float64\n",
      "id_06             136865 non-null float64\n",
      "id_07             5155 non-null float64\n",
      "id_08             5155 non-null float64\n",
      "id_09             74926 non-null float64\n",
      "id_10             74926 non-null float64\n",
      "id_11             140978 non-null float64\n",
      "id_12             144233 non-null object\n",
      "id_13             127320 non-null float64\n",
      "id_14             80044 non-null float64\n",
      "id_15             140985 non-null object\n",
      "id_16             129340 non-null object\n",
      "id_17             139369 non-null float64\n",
      "id_18             45113 non-null float64\n",
      "id_19             139318 non-null float64\n",
      "id_20             139261 non-null float64\n",
      "id_21             5159 non-null float64\n",
      "id_22             5169 non-null float64\n",
      "id_23             5169 non-null object\n",
      "id_24             4747 non-null float64\n",
      "id_25             5132 non-null float64\n",
      "id_26             5163 non-null float64\n",
      "id_27             5169 non-null object\n",
      "id_28             140978 non-null object\n",
      "id_29             140978 non-null object\n",
      "id_30             77565 non-null object\n",
      "id_31             140282 non-null object\n",
      "id_32             77586 non-null float64\n",
      "id_33             73289 non-null object\n",
      "id_34             77805 non-null object\n",
      "id_35             140985 non-null object\n",
      "id_36             140985 non-null object\n",
      "id_37             140985 non-null object\n",
      "id_38             140985 non-null object\n",
      "DeviceType        140810 non-null object\n",
      "DeviceInfo        118666 non-null object\n",
      "device_name       118666 non-null object\n",
      "device_version    31642 non-null object\n",
      "OS_id_30          77565 non-null object\n",
      "version_id_30     74745 non-null object\n",
      "browser_id_31     140282 non-null object\n",
      "version_id_31     138227 non-null object\n",
      "screen_width      73289 non-null object\n",
      "screen_height     73289 non-null object\n",
      "had_id            144233 non-null int64\n",
      "dtypes: float64(23), int64(2), object(25)\n",
      "memory usage: 55.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train_identity.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:57:09.416409Z",
     "start_time": "2019-08-24T08:56:57.453565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging data...\n",
      "Merged sucessful!\n",
      "\n",
      "Train dataset shape is (590540, 444)\n",
      "Test dataset shape is (506691, 443)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Merging data...')\n",
    "train_merge = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n",
    "test_merge = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n",
    "\n",
    "print('Merged sucessful!\\n')\n",
    "\n",
    "del train_identity, train_transaction, test_identity, test_transaction\n",
    "\n",
    "print(f'Train dataset shape is {train_merge.shape}')\n",
    "print(f'Test dataset shape is {test_merge.shape}')\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:57:10.489322Z",
     "start_time": "2019-08-24T08:57:10.485418Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_merge = reduce_mem_usage(train_merge)\n",
    "# test_merge = reduce_mem_usage(test_merge)\n",
    "# print(f\"Merged training set shape: {train_merge.shape}\")\n",
    "# print(f\"Merged testing set shape: {test_merge.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:57:11.484649Z",
     "start_time": "2019-08-24T08:57:11.478792Z"
    }
   },
   "outputs": [],
   "source": [
    "def high_duplicates_col(df, base_columns):\n",
    "    duplicates = []\n",
    "    i = 0\n",
    "    for c1 in base_columns:\n",
    "        i += 1\n",
    "        for c2 in base_columns[i: ]:\n",
    "            if c1 != c2:\n",
    "                if (np.sum((df[c1].values == \\\n",
    "                            df[c2].values).astype(int)) / len(df)) > 0.95:\n",
    "                    duplicates.append(c2)\n",
    "    \n",
    "    return list(set(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:57:12.413094Z",
     "start_time": "2019-08-24T08:57:12.406261Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_cols_to_drop(df, base_columns):\n",
    "    many_null_cols = [col for col in base_columns if df[col].isnull().sum() / df.shape[0] > 0.9]\n",
    "    big_top_value_cols = [col for col in base_columns if\n",
    "                          df[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n",
    "    cols_to_drop = list(set(many_null_cols + big_top_value_cols + high_duplicates_col(df, base_columns)))\n",
    "   \n",
    "    if 'isFraud' in cols_to_drop:\n",
    "        cols_to_drop.remove('isFraud')\n",
    "\n",
    "    return cols_to_drop\n",
    "\n",
    "\n",
    "def make_day_feature(df, timecol='TransactionDT'):\n",
    "    \"\"\"\n",
    "    Creates a day of the week feature, encoded as 0-6. \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        df to manipulate.\n",
    "    timecol : str\n",
    "        Name of the time column in df.\n",
    "    \"\"\"\n",
    "    days = df[timecol] / (3600*24)        \n",
    "    encoded_days = np.floor(days-1) % 7\n",
    "    return encoded_days\n",
    "\n",
    "def make_hour_feature(df, timecol='TransactionDT'):\n",
    "    \"\"\"\n",
    "    Creates an hour of the day feature, encoded as 0-23. \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        df to manipulate.\n",
    "    timecol : str\n",
    "        Name of the time column in df.\n",
    "    \"\"\"\n",
    "    hours = df[timecol] / (3600)        \n",
    "    encoded_hours = np.floor(hours) % 24\n",
    "    return encoded_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:57:13.358690Z",
     "start_time": "2019-08-24T08:57:13.355785Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cols_to_drop = get_cols_to_drop(train_merge, base_columns)\n",
    "# print(cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Around 40.8% of the transaction made by protonmail.com in `P_emaildomain` is fraudulent.\n",
    "*  95% of the transaction made by protonmail.com in `R_emaildomain` is fraudulent.\n",
    "* There are some links between fraud and the protonmail domain. Thus we can create features for this email domain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:57:17.230381Z",
     "start_time": "2019-08-24T08:57:17.069342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    95.121951\n",
      "0     4.878049\n",
      "Name: isFraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "fraud_count_r = train_merge.loc[train_merge['R_emaildomain'].isin(['protonmail.com']), 'isFraud'].value_counts()\n",
    "num_entries_r = len(train_merge.loc[train_merge['R_emaildomain'].isin(['protonmail.com']), 'isFraud'])\n",
    "fraud_percent_r = fraud_count_r / num_entries_r * 100\n",
    "print(fraud_percent_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:57:18.175441Z",
     "start_time": "2019-08-24T08:57:18.146162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    59.210526\n",
      "1    40.789474\n",
      "Name: isFraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "fraud_count = train_merge.loc[train_merge['P_emaildomain'].isin(['protonmail.com']), 'isFraud'].value_counts()\n",
    "num_entries = len(train_merge.loc[train_merge['P_emaildomain'].isin(['protonmail.com']), 'isFraud'])\n",
    "fraud_percent = fraud_count / num_entries * 100\n",
    "print(fraud_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:57:19.071903Z",
     "start_time": "2019-08-24T08:57:19.068973Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_merge['P_Isproton'] = (train_merge['P_emaildomain'] == 'protonmail.com')\n",
    "# train_merge['R_Isproton'] = (train_merge['R_emaildomain'] == 'protonmail.com')\n",
    "# test_merge['P_Isproton'] = (test_merge['P_emaildomain'] == 'protonmail.com')\n",
    "# test_merge['R_Isproton'] = (test_merge['R_emaildomain'] == 'protonmail.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:57:35.359115Z",
     "start_time": "2019-08-24T08:57:20.010902Z"
    }
   },
   "outputs": [],
   "source": [
    "train_merge['nulls_count'] = train_merge.isna().sum(axis=1)\n",
    "test_merge['nulls_count'] = test_merge.isna().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:57:38.008935Z",
     "start_time": "2019-08-24T08:57:36.171445Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest-579654\n",
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum',\n",
    "          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n",
    "          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n",
    "          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft',\n",
    "          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n",
    "          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n",
    "          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft',\n",
    "          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other',\n",
    "          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n",
    "          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n",
    "          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n",
    "          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink',\n",
    "          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other',\n",
    "          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo',\n",
    "          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other',\n",
    "          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other',\n",
    "          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other',\n",
    "          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n",
    "          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "us_emails = ['gmail', 'net', 'edu']\n",
    "\n",
    "for col in ['P_emaildomain', 'R_emaildomain']:\n",
    "    train_merge[col + '_bin'] = train_merge[col].map(emails)\n",
    "    test_merge[col + '_bin'] = test_merge[col].map (emails)\n",
    "    \n",
    "    train_merge[col + '_suffix'] = train_merge[col].map(lambda x: str(x).split('.')[-1])\n",
    "    test_merge[col + '_suffix'] = test_merge[col].map(lambda x: str(x).split('.')[-1])\n",
    "    \n",
    "    train_merge[col + '_suffix'] = train_merge[col + '_suffix'].map(\n",
    "        lambda x: x if str(x) not in us_emails else 'us')\n",
    "    test_merge[col + '_suffix'] = test_merge[col + '_suffix']. map(\n",
    "        lambda x: x if str(x) not in us_emails else 'us')\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Time of day has some dependency on the fraudulent transaction \n",
    "* Decimal part of the transaction amount maybe useful features\n",
    "* add client uID based on Card features and addr columns. need to remove it before modelling but can use it for aggregations features\n",
    "* Too many unique values for TransactionAmt, thus not generalize well. Use aggregations of features to reduce the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:57:39.346093Z",
     "start_time": "2019-08-24T08:57:39.273845Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/fchmiel/day-and-time-powerful-predictive-feature\n",
    "train_merge['Transaction_day'] = make_day_feature(train_merge)\n",
    "test_merge['Transaction_day'] = make_day_feature(test_merge)\n",
    "train_merge['Transaction_hour'] = make_hour_feature(train_merge)\n",
    "test_merge['Transaction_hour'] = make_hour_feature(test_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:57:42.844537Z",
     "start_time": "2019-08-24T08:57:40.510776Z"
    }
   },
   "outputs": [],
   "source": [
    "train_merge['uid'] = train_merge['card1'].astype(str) + \\\n",
    "    '_' + train_merge['card2'].astype(str) + \\\n",
    "    '_' + train_merge['card3'].astype(str) + \\\n",
    "    '_' + train_merge['card4'].astype(str)\n",
    "test_merge['uid'] = test_merge['card1'].astype(str) + \\\n",
    "    '_' + test_merge['card2'].astype(str) + \\\n",
    "    '_' + test_merge['card3'].astype(str) + \\\n",
    "    '_' + test_merge['card4'].astype(str)\n",
    "\n",
    "train_merge['uid2'] = train_merge['uid'].astype(str) + \\\n",
    "    '_' + train_merge['addr1'].astype(str) + '_' + \\\n",
    "    train_merge['addr2'].astype(str)\n",
    "test_merge['uid2'] = test_merge['uid'].astype(str) +  \\\n",
    "    '_' + test_merge['addr1'].astype(str) + '_' + \\\n",
    "    test_merge['addr2'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:58:38.377034Z",
     "start_time": "2019-08-24T08:57:44.240764Z"
    }
   },
   "outputs": [],
   "source": [
    "card_uid_cols = ['card1', 'card2', 'card3', 'card5', 'uid', 'uid2']\n",
    "\n",
    "for col in card_uid_cols:\n",
    "    for agg_type in ['mean', 'std']:\n",
    "        new_col_name = col + '_TransactionAmt_' + agg_type\n",
    "        temp_df = pd.concat([train_merge[[col, 'TransactionAmt']],\n",
    "                             test_merge[[col, 'TransactionAmt']]])\n",
    "        temp_df = temp_df.groupby([col])['TransactionAmt'].agg(\n",
    "            [agg_type]).reset_index().rename(columns={agg_type: new_col_name})\n",
    "        temp_df.index = list(temp_df[col])\n",
    "        temp_df = temp_df[new_col_name].to_dict()\n",
    "        train_merge[new_col_name] = train_merge[col].map(temp_df)\n",
    "        test_merge[new_col_name] = test_merge[col].map(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:58:43.925866Z",
     "start_time": "2019-08-24T08:58:39.217062Z"
    }
   },
   "outputs": [],
   "source": [
    "m_cols = ['M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9']\n",
    "train_merge['M_sum'] = train_merge[m_cols].sum(axis=1).astype(np.int8)\n",
    "test_merge['M_sum'] = test_merge[m_cols].sum(axis=1).astype(np.int8)\n",
    "\n",
    "train_merge['M_nulls'] = train_merge[m_cols].isna().sum(axis=1).astype(np.int8)\n",
    "test_merge['M_nulls'] = test_merge[m_cols].isna().sum(axis=1).astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* C columns are counts based on client information such as how many addresses are found to be associated with the payment card, etc\n",
    "* All of the c columns are dominated by either 0.0 or 1.0, some of the values are abusrdly large.  Its not common for clients to have more than 30 address for the payment card. So we can create feature to identify whether the transaction belongs to common value counts of C features \n",
    "* We can create some aggregate features based on these information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T08:58:48.126874Z",
     "start_time": "2019-08-24T08:58:44.500333Z"
    }
   },
   "outputs": [],
   "source": [
    "c_cols = train_merge.iloc[:,17:31].columns\n",
    "\n",
    "train_merge['C_sum'] = 0\n",
    "test_merge['C_sum'] = 0\n",
    "\n",
    "train_merge['C_null'] = 0\n",
    "test_merge['C_null'] = 0\n",
    "\n",
    "for cols in c_cols:\n",
    "    train_merge['C_sum'] += np.where(train_merge[col] == 1, 1, 0)\n",
    "    test_merge['C_sum'] += np.where(test_merge[col] == 1, 1, 0)\n",
    "    \n",
    "    train_merge['C_null'] += np.where(train_merge[col] == 0, 1, 0)\n",
    "    test_merge['C_null'] += np.where(test_merge[col] == 0, 1, 0)\n",
    "    \n",
    "    valid_values = train_merge[col].value_counts()\n",
    "    valid_values = valid_values[valid_values>1000]\n",
    "    valid_values = list(valid_values.index)\n",
    "    \n",
    "    train_merge[col + '_valid'] = np.where(train_merge[col].isin(valid_values), 1, 0)\n",
    "    test_merge[col + '_valid'] = np.where(test_merge[col].isin(valid_values), 1, 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T09:02:06.641941Z",
     "start_time": "2019-08-24T08:58:48.751818Z"
    }
   },
   "outputs": [],
   "source": [
    "freq_cols = ['card1', 'card2', 'card3', 'card5',\n",
    "          'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14',\n",
    "          'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9',\n",
    "          'addr1', 'addr2',\n",
    "          'dist1', 'dist2',\n",
    "          'P_emaildomain', 'R_emaildomain',\n",
    "          'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10',\n",
    "          'id_11', 'id_13', 'id_14', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_24',\n",
    "          'id_25', 'id_26', 'id_30', 'id_31', 'id_32', 'id_33',\n",
    "          'DeviceInfo']\n",
    "\n",
    "for col in freq_cols:\n",
    "    temp_df = pd.concat([train_merge[[col]], test_merge[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts().to_dict()\n",
    "    train_merge[col+'_fq_enc'] = train_merge[col].map(fq_encode)\n",
    "    test_merge[col+'_fq_enc'] = test_merge[col].map(fq_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T09:02:07.490070Z",
     "start_time": "2019-08-24T09:02:07.469547Z"
    }
   },
   "outputs": [],
   "source": [
    "# New feature - log of transaction amount. ()\n",
    "train_merge['TransactionAmt'] = np.log1p(train_merge['TransactionAmt'])\n",
    "test_merge['TransactionAmt'] = np.log1p(test_merge['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T09:02:49.342655Z",
     "start_time": "2019-08-24T09:02:08.182060Z"
    }
   },
   "outputs": [],
   "source": [
    "# Label Encode object columns\n",
    "for col in train_merge.columns:\n",
    "    if train_merge[col].dtype == 'O':\n",
    "        train_merge[col] = train_merge[col].astype(str)\n",
    "        test_merge[col] = test_merge[col].astype(str)\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train_merge[col]) + list(test_merge[col]))\n",
    "        train_merge[col] = le.transform(train_merge[col])\n",
    "        test_merge[col] = le.transform(test_merge[col])\n",
    "        \n",
    "        train_merge[col] = train_merge[col].astype('category')\n",
    "        test_merge[col] = test_merge[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T09:02:51.892981Z",
     "start_time": "2019-08-24T09:02:49.947055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 590540 entries, 0 to 590539\n",
      "Columns: 530 entries, TransactionID_x to DeviceInfo_fq_enc\n",
      "dtypes: category(45), float64(460), int32(1), int64(22), int8(2)\n",
      "memory usage: 2.2 GB\n"
     ]
    }
   ],
   "source": [
    "train_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T09:03:03.873205Z",
     "start_time": "2019-08-24T09:02:52.483699Z"
    }
   },
   "outputs": [],
   "source": [
    "features_check = []\n",
    "columns_to_check = set(list(train_merge)).difference(base_columns)\n",
    "for col in columns_to_check:\n",
    "    features_check.append(ks_2samp(test_merge[col], train_merge[col])[1])\n",
    "\n",
    "features_check = pd.Series(features_check, index=columns_to_check).sort_values() \n",
    "features_discard = list(features_check[features_check==0].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T09:03:04.374090Z",
     "start_time": "2019-08-24T09:03:04.370186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M_nulls', 'version_id_31', 'C12_fq_enc', 'D5_fq_enc', 'id_31_fq_enc', 'D4_fq_enc', 'D7_fq_enc', 'D3_fq_enc', 'D6_fq_enc', 'id_13_fq_enc', 'nulls_count', 'TransactionID_y', 'id_20_fq_enc', 'TransactionID_x']\n"
     ]
    }
   ],
   "source": [
    "print(features_discard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T09:03:04.900594Z",
     "start_time": "2019-08-24T09:03:04.893763Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_to_drop = ['V112', 'V315', 'V293', 'id_25', 'V135', 'V136', 'V284', 'V298', 'V300', 'V316',\n",
    "                'V111', 'dist2', 'V105', 'V113', 'V104', 'id_24', 'id_22', 'V117', 'V121', 'V125',\n",
    "                'V320', 'V103', 'V109', 'V118', 'V295', 'V303', 'V119', 'V134', 'V106', 'V281',\n",
    "                'V120', 'V290', 'V98', 'V102', 'V115', 'V137', 'V123', 'id_08', 'V309', 'id_18',\n",
    "                'V114', 'V321', 'V116', 'V133', 'V108', 'V301', 'V124', 'C3', 'V296', 'id_23',\n",
    "                'V122', 'V129', 'id_26', 'V304', 'V110', 'V107', 'id_21', 'V286',  'id_27',\n",
    "                'V297', 'V299', 'V311', 'V319', 'V305', 'V101', 'V289', 'id_07', 'V132', 'V318', 'D7',\n",
    "                'V241', 'V89', 'V68', 'V65', 'OS_id_30', 'C_null', 'C_sum',\n",
    "                'M_sum', 'V88', 'V41', 'V28', 'V240', 'V27', 'V1', 'V14', 'id_24_fq_enc', 'V325', 'V138',\n",
    "                'browser_id_31', 'V142', 'id_25_fq_enc', 'id_22_fq_enc', 'V141', 'V269']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T09:03:05.402399Z",
     "start_time": "2019-08-24T09:03:05.398493Z"
    }
   },
   "outputs": [],
   "source": [
    "features_discard = features_discard + cols_to_drop + ['uid', 'uid2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T09:03:05.908051Z",
     "start_time": "2019-08-24T09:03:05.904148Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_discard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T09:03:10.227487Z",
     "start_time": "2019-08-24T09:03:06.407478Z"
    }
   },
   "outputs": [],
   "source": [
    "train_merge = train_merge.drop(features_discard, axis=1) \n",
    "test_merge = test_merge.drop(features_discard, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T09:03:10.768315Z",
     "start_time": "2019-08-24T09:03:10.746335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 590540 entries, 0 to 590539\n",
      "Columns: 420 entries, isFraud to DeviceInfo_fq_enc\n",
      "dtypes: category(38), float64(364), int32(1), int64(17)\n",
      "memory usage: 1.7 GB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506691 entries, 0 to 506690\n",
      "Columns: 419 entries, TransactionDT to DeviceInfo_fq_enc\n",
      "dtypes: category(38), float64(377), int32(1), int64(3)\n",
      "memory usage: 1.5 GB\n"
     ]
    }
   ],
   "source": [
    "train_merge.info()\n",
    "test_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T09:03:16.190406Z",
     "start_time": "2019-08-24T09:03:11.168759Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_merge.sort_values('TransactionDT').drop(['isFraud',\n",
    "                                                         'TransactionDT'\n",
    "                                                         ],\n",
    "                                                       axis=1)\n",
    "y_train = train_merge.sort_values('TransactionDT')['isFraud']\n",
    "X_test = test_merge.sort_values('TransactionDT').drop(['TransactionDT'],\n",
    "                                                      axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T09:03:17.335762Z",
     "start_time": "2019-08-24T09:03:17.318195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 590540 entries, 0 to 590539\n",
      "Columns: 418 entries, TransactionAmt to DeviceInfo_fq_enc\n",
      "dtypes: category(38), float64(364), int32(1), int64(15)\n",
      "memory usage: 1.7 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(590540,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.info()\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T09:03:18.205335Z",
     "start_time": "2019-08-24T09:03:18.194599Z"
    }
   },
   "outputs": [],
   "source": [
    "data_types_train = X_train.dtypes.astype(str).to_dict()\n",
    "\n",
    "data_types_test = X_test.dtypes.astype(str).to_dict()\n",
    "with open('../preprocessed_data/data_type_key_train.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_types_train, f)\n",
    "with open('../preprocessed_data/data_type_key_test.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_types_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T09:07:21.274267Z",
     "start_time": "2019-08-24T09:03:18.982863Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.to_csv(\"../preprocessed_data/X_train.csv\", index=False)\n",
    "y_train.to_csv(\"../preprocessed_data/y_train.csv\", index=False, header=False)\n",
    "X_test.to_csv(\"../preprocessed_data/X_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T09:07:22.133741Z",
     "start_time": "2019-08-24T09:07:22.129839Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../preprocessed_data/data_type_key_test.json') as data_file:    \n",
    "    data_types = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T09:07:49.158472Z",
     "start_time": "2019-08-24T09:07:22.927238Z"
    }
   },
   "outputs": [],
   "source": [
    "train_X = pd.read_csv(\"../preprocessed_data/X_train.csv\", dtype=data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T09:07:50.229163Z",
     "start_time": "2019-08-24T09:07:50.217472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 590540 entries, 0 to 590539\n",
      "Columns: 418 entries, TransactionAmt to DeviceInfo_fq_enc\n",
      "dtypes: category(38), float64(377), int32(1), int64(2)\n",
      "memory usage: 1.7 GB\n"
     ]
    }
   ],
   "source": [
    "train_X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T09:07:51.269349Z",
     "start_time": "2019-08-24T09:07:51.220526Z"
    }
   },
   "outputs": [],
   "source": [
    "train_y = pd.read_csv(\"../preprocessed_data/y_train.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
