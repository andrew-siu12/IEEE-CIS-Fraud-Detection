{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:28:56.020971Z",
     "start_time": "2019-08-19T13:28:54.230012Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.util import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from time import time\n",
    "import datetime\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:29:58.022653Z",
     "start_time": "2019-08-19T13:28:56.183130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_identity = pd.read_csv(\"data/train_identity.csv\", )\n",
    "train_transaction = pd.read_csv(\"data/train_transaction.csv\", )\n",
    "test_identity = pd.read_csv(\"data/test_identity.csv\", )\n",
    "test_transaction = pd.read_csv(\"data/test_transaction.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:30:10.935139Z",
     "start_time": "2019-08-19T13:30:10.926355Z"
    }
   },
   "outputs": [],
   "source": [
    "base_columns = list(train_transaction.columns) + list(train_identity.columns)\n",
    "# base model feature improtance\n",
    "feat = ['card2', 'C13', 'card1', 'TransactionAmt', 'C1', 'addr1', 'D15',\n",
    "        'D2', 'P_emaildomain', 'C14', 'card5', 'C11', 'V45', 'D8',\n",
    "        'card3', 'V313', 'D1', 'id_02', 'R_emaildomain', 'card6',\n",
    "        'id_20', 'D4', 'D10', 'DeviceInfo', 'C2', 'id_01', 'V310',\n",
    "        'V62', 'C12', 'dist1', 'V87', 'M4', 'V283', 'V281', 'V294',\n",
    "        'V258', 'C8', 'V53', 'id_09', 'V314', 'V38', 'id_30', 'V315',\n",
    "        'C6', 'V317', 'id_33', 'V312', 'V189', 'id_19', 'C4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Group same mobile phone company with different build into same group\n",
    "* Fill missing values with unknown device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:30:25.256969Z",
     "start_time": "2019-08-19T13:30:23.823224Z"
    }
   },
   "outputs": [],
   "source": [
    "train_identity['DeviceInfo'] = train_identity['DeviceInfo'].fillna(\n",
    "    'unknown_device').str.lower()\n",
    "test_identity['DeviceInfo'] = test_identity['DeviceInfo'].fillna(\n",
    "    'unknown_device').str.lower()\n",
    "\n",
    "train_identity['DeviceInfo_c'] = train_identity['DeviceInfo']\n",
    "test_identity['DeviceInfo_c'] = test_identity['DeviceInfo']\n",
    "\n",
    "device_match_dict = {\n",
    "    'sm': 'sm-',\n",
    "    'sm': 'samsung',\n",
    "    'huawei': 'huawei',\n",
    "    'moto': 'moto',\n",
    "    'rv': 'rv:',\n",
    "    'trident': 'trident',\n",
    "    'lg': 'lg-',\n",
    "    'htc': 'htc',\n",
    "    'blade': 'blade',\n",
    "    'windows': 'windows',\n",
    "    'lenovo': 'lenovo',\n",
    "    'linux': 'linux',\n",
    "    'f3': 'f3',\n",
    "    'f5': 'f5'\n",
    "}\n",
    "\n",
    "for dev_type_s, dev_type_o in device_match_dict.items():\n",
    "    train_identity['DeviceInfo_c'] = train_identity['DeviceInfo_c'].apply(\n",
    "        lambda x: dev_type_s if dev_type_o in x else x)\n",
    "    test_identity['DeviceInfo_c'] = test_identity['DeviceInfo_c'].apply(\n",
    "        lambda x: dev_type_s if dev_type_o in x else x)\n",
    "\n",
    "train_identity['DeviceInfo_c'] = train_identity['DeviceInfo_c'].apply(\n",
    "    lambda x: 'other_d_type' if x not in device_match_dict else x)\n",
    "test_identity['DeviceInfo_c'] = test_identity['DeviceInfo_c'].apply(\n",
    "    lambda x: 'other_d_type' if x not in device_match_dict else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Group same desktop os company with different build into same group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:30:40.206367Z",
     "start_time": "2019-08-19T13:30:39.049807Z"
    }
   },
   "outputs": [],
   "source": [
    "train_identity['id_30'] = train_identity['id_30'].fillna(\n",
    "    'unknown_device').str.lower()\n",
    "test_identity['id_30'] = test_identity['id_30'].fillna(\n",
    "    'unknown_device').str.lower()\n",
    "\n",
    "train_identity['id_30_c'] = train_identity['id_30']\n",
    "test_identity['id_30_c'] = test_identity['id_30']\n",
    "\n",
    "device_match_dict = {\n",
    "    'ios':'ios',\n",
    "    'windows':'windows',\n",
    "    'mac':'mac',\n",
    "    'android':'android'\n",
    "}\n",
    "\n",
    "for dev_type_s, dev_type_o in device_match_dict.items():\n",
    "    train_identity['id_30_c'] = train_identity['id_30_c'].apply(\n",
    "        lambda x: dev_type_s if dev_type_o in x else x)\n",
    "    test_identity['id_30_c'] = test_identity['id_30_c'].apply(\n",
    "        lambda x: dev_type_s if dev_type_o in x else x)\n",
    "\n",
    "train_identity['id_30_v'] = train_identity['id_30'].apply(\n",
    "    lambda x: ''.join([i for i in x if i.isdigit()]))\n",
    "test_identity['id_30_v'] = test_identity['id_30'].apply(\n",
    "    lambda x: ''.join([i for i in x if i.isdigit()]))\n",
    "\n",
    "train_identity['id_30_v'] = np.where(\n",
    "    train_identity['id_30_v'] != '', train_identity['id_30_v'], 0).astype(int)\n",
    "test_identity['id_30_v'] = np.where(\n",
    "    test_identity['id_30_v'] != '', test_identity['id_30_v'], 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* make browser build and build number as seperate feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:30:54.709734Z",
     "start_time": "2019-08-19T13:30:54.299813Z"
    }
   },
   "outputs": [],
   "source": [
    "train_identity['id_31'] = train_identity['id_31'].fillna(\n",
    "    'unknown_br').str.lower()\n",
    "test_identity['id_31'] = test_identity['id_31'].fillna(\n",
    "    'unknown_br').str.lower()\n",
    "\n",
    "train_identity['id_31'] = train_identity['id_31'].apply(\n",
    "    lambda x: x.replace('webview', 'webvw'))\n",
    "test_identity['id_31'] = test_identity['id_31'].apply(\n",
    "    lambda x: x.replace('webview', 'webvw'))\n",
    "\n",
    "train_identity['id_31'] = train_identity['id_31'].apply(\n",
    "    lambda x: x.replace('for', ' '))\n",
    "test_identity['id_31'] = test_identity['id_31'].apply(\n",
    "    lambda x: x.replace('for', ' '))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:31:32.140326Z",
     "start_time": "2019-08-19T13:31:08.814892Z"
    }
   },
   "outputs": [],
   "source": [
    "browser_list = set(\n",
    "    list(train_identity['id_31'].unique()) + list(test_identity['id_31'].unique()))\n",
    "browser_list2 = []\n",
    "for item in browser_list:\n",
    "    browser_list2 += item.split(' ')\n",
    "browser_list2 = list(set(browser_list2))\n",
    "\n",
    "browser_list3 = []\n",
    "for item in browser_list2:\n",
    "    browser_list3 += item.split('/')\n",
    "browser_list3 = list(set(browser_list3))\n",
    "\n",
    "for item in browser_list3:\n",
    "    train_identity['id_31_e_'+item] = np.where(\n",
    "        train_identity['id_31'].str.contains(item), 1, 0).astype(np.int8)\n",
    "    test_identity['id_31_e_'+item] = np.where(\n",
    "        test_identity['id_31'].str.contains(item), 1, 0).astype(np.int8)\n",
    "    if train_identity['id_31_e_'+item].sum() < 100:\n",
    "        del train_identity['id_31_e_'+item], test_identity['id_31_e_'+item]\n",
    "\n",
    "train_identity['id_31_v'] = train_identity['id_31'].apply(\n",
    "    lambda x: ''.join([i for i in x if i.isdigit()]))\n",
    "test_identity['id_31_v'] = test_identity['id_31'].apply(\n",
    "    lambda x: ''.join([i for i in x if i.isdigit()]))\n",
    "\n",
    "train_identity['id_31_v'] = np.where(\n",
    "    train_identity['id_31_v'] != '', train_identity['id_31_v'], 0).astype(int)\n",
    "test_identity['id_31_v'] = np.where(\n",
    "    test_identity['id_31_v'] != '', test_identity['id_31_v'], 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:31:56.319761Z",
     "start_time": "2019-08-19T13:31:43.805483Z"
    }
   },
   "outputs": [],
   "source": [
    "## Merge identity and transactions columns\n",
    "temp_train_df = train_transaction[['TransactionID']]\n",
    "temp_train_df = temp_train_df.merge(train_identity, on=['TransactionID'], how='left')\n",
    "del temp_train_df['TransactionID']\n",
    "train_merge = pd.concat([train_transaction, temp_train_df], axis=1)\n",
    "del temp_train_df\n",
    "\n",
    "temp_test_df = test_transaction[['TransactionID']]\n",
    "temp_test_df = temp_test_df.merge(test_identity, on=['TransactionID'], how='left')\n",
    "del temp_test_df['TransactionID']\n",
    "test_merge = pd.concat([test_transaction, temp_test_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:32:25.253293Z",
     "start_time": "2019-08-19T13:32:24.578876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_transaction, train_identity, test_transaction, test_identity\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:38:29.471896Z",
     "start_time": "2019-08-19T13:32:41.131844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 714.68 Mb (67.6% reduction)\n",
      "Mem. usage decreased to 620.45 Mb (67.1% reduction)\n",
      "Merged training set shape: (590540, 489)\n",
      "Merged testing set shape: (506691, 488)\n"
     ]
    }
   ],
   "source": [
    "train_merge = reduce_mem_usage(train_merge)\n",
    "test_merge = reduce_mem_usage(test_merge)\n",
    "print(f\"Merged training set shape: {train_merge.shape}\")\n",
    "print(f\"Merged testing set shape: {test_merge.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:38:42.799835Z",
     "start_time": "2019-08-19T13:38:42.792026Z"
    }
   },
   "outputs": [],
   "source": [
    "def high_duplicates_col(df, base_columns):\n",
    "    duplicates = []\n",
    "    i = 0\n",
    "    for c1 in base_columns:\n",
    "        i += 1\n",
    "        for c2 in base_columns[i: ]:\n",
    "            if c1 != c2:\n",
    "                if (np.sum((df[c1].values == \\\n",
    "                            df[c2].values).astype(int)) / len(df)) > 0.95:\n",
    "                    duplicates.append(c2)\n",
    "    \n",
    "    return list(set(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:38:56.083994Z",
     "start_time": "2019-08-19T13:38:56.067872Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_cols_to_drop(df, base_columns):\n",
    "    many_null_cols = [col for col in base_columns if df[col].isnull().sum() / df.shape[0] > 0.9]\n",
    "    big_top_value_cols = [col for col in base_columns if\n",
    "                          df[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n",
    "    cols_to_drop = list(set(many_null_cols + big_top_value_cols + high_duplicates_col(df, base_columns)))\n",
    "   \n",
    "    if 'isFraud' in cols_to_drop:\n",
    "        cols_to_drop.remove('isFraud')\n",
    "\n",
    "    return cols_to_drop\n",
    "\n",
    "\n",
    "def make_day_feature(df, timecol='TransactionDT'):\n",
    "    \"\"\"\n",
    "    Creates a day of the week feature, encoded as 0-6. \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        df to manipulate.\n",
    "    timecol : str\n",
    "        Name of the time column in df.\n",
    "    \"\"\"\n",
    "    days = df[timecol] / (3600*24)        \n",
    "    encoded_days = np.floor(days-1) % 7\n",
    "    return encoded_days\n",
    "\n",
    "def make_hour_feature(df, timecol='TransactionDT'):\n",
    "    \"\"\"\n",
    "    Creates an hour of the day feature, encoded as 0-23. \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        df to manipulate.\n",
    "    timecol : str\n",
    "        Name of the time column in df.\n",
    "    \"\"\"\n",
    "    hours = df[timecol] / (3600)        \n",
    "    encoded_hours = np.floor(hours) % 24\n",
    "    return encoded_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:52:30.962367Z",
     "start_time": "2019-08-19T13:39:09.593971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V112', 'V315', 'V293', 'id_25', 'V135', 'V136', 'V284', 'V298', 'V300', 'V316', 'V111', 'dist2', 'V105', 'V113', 'V104', 'id_24', 'id_22', 'V117', 'V121', 'V125', 'V320', 'V103', 'V109', 'V118', 'V295', 'V303', 'V119', 'V134', 'V106', 'V281', 'V120', 'V290', 'V98', 'V102', 'V115', 'V137', 'V123', 'id_08', 'V309', 'id_18', 'V114', 'V321', 'V116', 'V133', 'V108', 'V301', 'V124', 'C3', 'V296', 'id_23', 'V122', 'V129', 'id_26', 'V304', 'V110', 'V107', 'id_21', 'V286', 'id_27', 'V297', 'V299', 'V311', 'V319', 'V305', 'V101', 'V289', 'id_07', 'V132', 'V318', 'D7']\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = get_cols_to_drop(train_merge, base_columns)\n",
    "print(cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['V112', 'V315', 'V293', 'id_25', 'V135', 'V136', 'V284', 'V298', 'V300', 'V316', \n",
    "#  'V111', 'dist2', 'V105', 'V113', 'V104', 'id_24', 'id_22', 'V117', 'V121', 'V125', \n",
    "#  'V320', 'V103', 'V109', 'V118', 'V295', 'V303', 'V119', 'V134', 'V106', 'V281', \n",
    "#  'V120', 'V290', 'V98', 'V102', 'V115', 'V137', 'V123', 'id_08', 'V309', 'id_18', \n",
    "#  'V114', 'V321', 'V116', 'V133', 'V108', 'V301', 'V124', 'C3', 'V296', 'id_23', \n",
    "#  'V122', 'V129', 'id_26', 'V304', 'V110', 'V107', 'id_21', 'V286',  'id_27', \n",
    "#  'V297', 'V299', 'V311', 'V319', 'V305', 'V101', 'V289', 'id_07', 'V132', 'V318', 'D7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Around 40.8% of the transaction made by protonmail.com in `P_emaildomain` is fraudulent.\n",
    "*  95% of the transaction made by protonmail.com in `R_emaildomain` is fraudulent.\n",
    "* There are some links between fraud and the protonmail domain. Thus we can create features for this email domain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:55:19.242630Z",
     "start_time": "2019-08-19T13:55:19.201606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    95.121951\n",
      "0     4.878049\n",
      "Name: isFraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "fraud_count_r = train_merge.loc[train_merge['R_emaildomain'].isin(['protonmail.com']), 'isFraud'].value_counts()\n",
    "num_entries_r = len(train_merge.loc[train_merge['R_emaildomain'].isin(['protonmail.com']), 'isFraud'])\n",
    "fraud_percent_r = fraud_count_r / num_entries_r * 100\n",
    "print(fraud_percent_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:55:32.440703Z",
     "start_time": "2019-08-19T13:55:32.403581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    59.210526\n",
      "1    40.789474\n",
      "Name: isFraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "fraud_count = train_merge.loc[train_merge['P_emaildomain'].isin(['protonmail.com']), 'isFraud'].value_counts()\n",
    "num_entries = len(train_merge.loc[train_merge['P_emaildomain'].isin(['protonmail.com']), 'isFraud'])\n",
    "fraud_percent = fraud_count / num_entries * 100\n",
    "print(fraud_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:55:47.921017Z",
     "start_time": "2019-08-19T13:55:45.600616Z"
    }
   },
   "outputs": [],
   "source": [
    "train_merge['P_Isproton'] = (train_merge['P_emaildomain'] == 'protonmail.com')\n",
    "train_merge['R_Isproton'] = (train_merge['R_emaildomain'] == 'protonmail.com')\n",
    "test_merge['P_Isproton'] = (test_merge['P_emaildomain'] == 'protonmail.com')\n",
    "test_merge['R_Isproton'] = (test_merge['R_emaildomain'] == 'protonmail.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:56:10.117147Z",
     "start_time": "2019-08-19T13:55:58.229193Z"
    }
   },
   "outputs": [],
   "source": [
    "train_merge['nulls_count'] = train_merge.isna().sum(axis=1)\n",
    "test_merge['nulls_count'] = test_merge.isna().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:56:23.893927Z",
     "start_time": "2019-08-19T13:56:20.513283Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest-579654\n",
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum',\n",
    "          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n",
    "          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n",
    "          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft',\n",
    "          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n",
    "          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n",
    "          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft',\n",
    "          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other',\n",
    "          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n",
    "          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n",
    "          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n",
    "          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink',\n",
    "          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other',\n",
    "          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo',\n",
    "          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other',\n",
    "          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other',\n",
    "          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other',\n",
    "          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n",
    "          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "us_emails = ['gmail', 'net', 'edu']\n",
    "\n",
    "for col in ['P_emaildomain', 'R_emaildomain']:\n",
    "    train_merge[col + '_bin'] = train_merge[col].map(emails)\n",
    "    test_merge[col + '_bin'] = test_merge[col].map (emails)\n",
    "    \n",
    "    train_merge[col + '_suffix'] = train_merge[col].map(lambda x: str(x).split('.')[-1])\n",
    "    test_merge[col + '_suffix'] = test_merge[col].map(lambda x: str(x).split('.')[-1])\n",
    "    \n",
    "    train_merge[col + '_suffix'] = train_merge[col + '_suffix'].map(\n",
    "        lambda x: x if str(x) not in us_emails else 'us')\n",
    "    test_merge[col + '_suffix'] = test_merge[col + '_suffix']. map(\n",
    "        lambda x: x if str(x) not in us_emails else 'us')\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Time of day has some dependency on the fraudulent transaction \n",
    "* Decimal part of the transaction amount maybe useful features\n",
    "* add client uID based on Card features and addr columns. need to remove it before modelling but can use it for aggregations features\n",
    "* Too many unique values for TransactionAmt, thus not generalize well. Use aggregations of features to reduce the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:56:40.049287Z",
     "start_time": "2019-08-19T13:56:39.932057Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/fchmiel/day-and-time-powerful-predictive-feature\n",
    "train_merge['Transaction_day'] = make_day_feature(train_merge)\n",
    "test_merge['Transaction_day'] = make_day_feature(test_merge)\n",
    "train_merge['Transaction_hour'] = make_hour_feature(train_merge)\n",
    "test_merge['Transaction_hour'] = make_hour_feature(test_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:57:01.763454Z",
     "start_time": "2019-08-19T13:56:57.141291Z"
    }
   },
   "outputs": [],
   "source": [
    "train_merge['uid'] = train_merge['card1'].astype(str) + \\\n",
    "    '_' + train_merge['card2'].astype(str) + \\\n",
    "    '_' + train_merge['card3'].astype(str) + \\\n",
    "    '_' + train_merge['card4'].astype(str)\n",
    "test_merge['uid'] = test_merge['card1'].astype(str) + \\\n",
    "    '_' + test_merge['card2'].astype(str) + \\\n",
    "    '_' + test_merge['card3'].astype(str) + \\\n",
    "    '_' + test_merge['card4'].astype(str)\n",
    "\n",
    "train_merge['uid2'] = train_merge['uid'].astype(str) + \\\n",
    "    '_' + train_merge['addr1'].astype(str) + '_' + \\\n",
    "    train_merge['addr2'].astype(str)\n",
    "test_merge['uid2'] = test_merge['uid'].astype(str) +  \\\n",
    "    '_' + test_merge['addr1'].astype(str) + '_' + \\\n",
    "    test_merge['addr2'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:57:31.462601Z",
     "start_time": "2019-08-19T13:57:24.117589Z"
    }
   },
   "outputs": [],
   "source": [
    "card_uid_cols = ['card1', 'card2', 'card3', 'card5', 'uid', 'uid2']\n",
    "\n",
    "for col in card_uid_cols:\n",
    "    for agg_type in ['mean', 'std']:\n",
    "        new_col_name = col + '_TransactionAmt_' + agg_type\n",
    "        temp_df = pd.concat([train_merge[[col, 'TransactionAmt']],\n",
    "                             test_merge[[col, 'TransactionAmt']]])\n",
    "        temp_df = temp_df.groupby([col])['TransactionAmt'].agg(\n",
    "            [agg_type]).reset_index().rename(columns={agg_type: new_col_name})\n",
    "        temp_df.index = list(temp_df[col])\n",
    "        temp_df = temp_df[new_col_name].to_dict()\n",
    "        train_merge[new_col_name] = train_merge[col].map(temp_df)\n",
    "        test_merge[new_col_name] = test_merge[col].map(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:57:48.395882Z",
     "start_time": "2019-08-19T13:57:46.282590Z"
    }
   },
   "outputs": [],
   "source": [
    "m_cols = ['M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9']\n",
    "train_merge['M_sum'] = train_merge[m_cols].sum(axis=1).astype(np.int8)\n",
    "test_merge['M_sum'] = test_merge[m_cols].sum(axis=1).astype(np.int8)\n",
    "\n",
    "train_merge['M_nulls'] = train_merge[m_cols].isna().sum(axis=1).astype(np.int8)\n",
    "test_merge['M_nulls'] = test_merge[m_cols].isna().sum(axis=1).astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* C columns are counts based on client information such as how many addresses are found to be associated with the payment card, etc\n",
    "* All of the c columns are dominated by either 0.0 or 1.0, some of the values are abusrdly large.  Its not common for clients to have more than 30 address for the payment card. So we can create feature to identify whether the transaction belongs to common value counts of C features \n",
    "* We can create some aggregate features based on these information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:58:06.596011Z",
     "start_time": "2019-08-19T13:58:00.154061Z"
    }
   },
   "outputs": [],
   "source": [
    "c_cols = train_merge.iloc[:,17:31].columns\n",
    "\n",
    "train_merge['C_sum'] = 0\n",
    "test_merge['C_sum'] = 0\n",
    "\n",
    "train_merge['C_null'] = 0\n",
    "test_merge['C_null'] = 0\n",
    "\n",
    "for cols in c_cols:\n",
    "    train_merge['C_sum'] += np.where(train_merge[col] == 1, 1, 0)\n",
    "    test_merge['C_sum'] += np.where(test_merge[col] == 1, 1, 0)\n",
    "    \n",
    "    train_merge['C_null'] += np.where(train_merge[col] == 0, 1, 0)\n",
    "    test_merge['C_null'] += np.where(test_merge[col] == 0, 1, 0)\n",
    "    \n",
    "    valid_values = train_merge[col].value_counts()\n",
    "    valid_values = valid_values[valid_values>1000]\n",
    "    valid_values = list(valid_values.index)\n",
    "    \n",
    "    train_merge[col + '_valid'] = np.where(train_merge[col].isin(valid_values), 1, 0)\n",
    "    test_merge[col + '_valid'] = np.where(test_merge[col].isin(valid_values), 1, 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:59:00.837036Z",
     "start_time": "2019-08-19T13:58:18.965176Z"
    }
   },
   "outputs": [],
   "source": [
    "freq_cols = ['card1', 'card2', 'card3', 'card5',\n",
    "          'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14',\n",
    "          'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9',\n",
    "          'addr1', 'addr2',\n",
    "          'dist1', 'dist2',\n",
    "          'P_emaildomain', 'R_emaildomain',\n",
    "          'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10',\n",
    "          'id_11', 'id_13', 'id_14', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_24',\n",
    "          'id_25', 'id_26', 'id_30', 'id_31', 'id_32', 'id_33',\n",
    "          'DeviceInfo', 'DeviceInfo_c', 'id_30_c', 'id_30_v', 'id_31_v',\n",
    "          ]\n",
    "\n",
    "for col in freq_cols:\n",
    "    temp_df = pd.concat([train_merge[[col]], test_merge[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts().to_dict()\n",
    "    train_merge[col+'_fq_enc'] = train_merge[col].map(fq_encode)\n",
    "    test_merge[col+'_fq_enc'] = test_merge[col].map(fq_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T14:00:09.654689Z",
     "start_time": "2019-08-19T13:59:14.013621Z"
    }
   },
   "outputs": [],
   "source": [
    "# Label Encode object columns\n",
    "for col in train_merge.columns:\n",
    "    if train_merge[col].dtype == 'O':\n",
    "        train_merge[col] = train_merge[col].astype(str)\n",
    "        test_merge[col] = test_merge[col].astype(str)\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train_merge[col]) + list(test_merge[col]))\n",
    "        train_merge[col] = le.transform(train_merge[col])\n",
    "        test_merge[col] = le.transform(test_merge[col])\n",
    "        \n",
    "        train_merge[col] = train_merge[col].astype('category')\n",
    "        test_merge[col] = test_merge[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T14:00:49.520536Z",
     "start_time": "2019-08-19T14:00:21.467165Z"
    }
   },
   "outputs": [],
   "source": [
    "features_check = []\n",
    "columns_to_check = set(list(train_merge)).difference(base_columns)\n",
    "for col in columns_to_check:\n",
    "    features_check.append(ks_2samp(test_merge[col], train_merge[col])[1])\n",
    "\n",
    "features_check = pd.Series(features_check, index=columns_to_check).sort_values() \n",
    "features_discard = list(features_check[features_check==0].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T14:01:01.246068Z",
     "start_time": "2019-08-19T14:01:01.241187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id_31_e_4.0', 'id_31_e_58.0', 'id_31_e_15.0', 'id_31_e_62.0', 'M_nulls', 'id_31_e_16.0', 'id_31_fq_enc', 'D5_fq_enc', 'id_20_fq_enc', 'D7_fq_enc', 'id_31_e_generic', 'id_31_e_63.0', 'id_31_e_11.0', 'id_31_e_6.2', 'C12_fq_enc', 'id_31_e_desktop', 'nulls_count', 'D4_fq_enc', 'id_31_e_65.0', 'id_31_e_64.0', 'id_13_fq_enc', 'id_31_e_66.0', 'id_31_v_fq_enc', 'id_31_e_59.0', 'D3_fq_enc', 'D6_fq_enc', 'id_31_e_57.0', 'id_31_v', 'id_31_e_ie']\n"
     ]
    }
   ],
   "source": [
    "print(features_discard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T14:15:06.949926Z",
     "start_time": "2019-08-19T14:15:06.946023Z"
    }
   },
   "outputs": [],
   "source": [
    "features_discard = features_discard + cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T14:17:54.878509Z",
     "start_time": "2019-08-19T14:17:52.277344Z"
    }
   },
   "outputs": [],
   "source": [
    "train_merge = train_merge.drop(features_discard, axis=1) \n",
    "test_merge = test_merge.drop(features_discard, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T14:28:34.233691Z",
     "start_time": "2019-08-19T14:20:10.470473Z"
    }
   },
   "outputs": [],
   "source": [
    "train_merge.to_csv(\"preprocessed_data/train.csv\", index=False)\n",
    "test_merge.to_csv(\"preprocessed_data/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
